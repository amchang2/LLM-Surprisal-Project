{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amchang2/LLM-Surprisal-Project/blob/main/Final_Project_Cogs_150.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ab835a"
      },
      "source": [
        "# Final Project Cogs 150\n",
        "\n",
        "**Amanda Chang**\n",
        "\n",
        "*Spring 2024*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Research question: Are LLMs sensitive to negation?\n",
        "\n",
        "To explore an LLM's sensitivity to negation thoroughly, these questions were addressed when creating the stimuli:\n",
        "\n",
        "- Do LLMs display consistently higher **surprisal** when presented with `semantically similar` words or with `logical opposites` in negation tasks?\n",
        "-  Do LLMs display different  **surprisal** values to a target word depending on whether it follows an `affirmative` or `negated` sentence?"
      ],
      "metadata": {
        "id": "4QgxdfUxvKQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers Set Up"
      ],
      "metadata": {
        "id": "JPF2Iz64HSMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and load `transformers`"
      ],
      "metadata": {
        "id": "j953UzZxfbY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers"
      ],
      "metadata": {
        "id": "qIx70Cnsfc-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'  # makes figs nicer!"
      ],
      "metadata": {
        "id": "EJyLEki9i82t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "oG8syt-ugpfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load a pre-trained model\n",
        "\n",
        "We can use the `from_pretrained` function to load an existing, pre-trained model (GPT-2)."
      ],
      "metadata": {
        "id": "405AuASgpbn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers"
      ],
      "metadata": {
        "id": "HjS1TU5jpzQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2 = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")  # Load the model\n",
        "gpt2.eval()  # Put the model in \"evaluation mode\" (as opposed to training mode)."
      ],
      "metadata": {
        "id": "d6XJHIZChBQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Psycholinguistic stimuli\n",
        "\n",
        "Research question: Are LLMs sensitive to negation?\n",
        "\n",
        "To test this, we'd need to devise *pairs* of stimuli:\n",
        "\n",
        "- We'll run each of these through GPT-2.  \n",
        "\n",
        "**Semantically similar words vs. Logical opposites:**\n",
        "- For each \"minimal pair\" (`The weather today is not cloudy, it is`), we'll want two versions, which end with either a `semantically similar` word (` foggy`) or a `logical opposite` (` sunny`).\n",
        "- Final words are reused with a different sentence frame that inverts the logistically opposite pattern.\n",
        "- If the LLM is sensitive to negation, the surprisal relationships should reflect that `semantically similar` and `same word` completions consistently have higher surprisal than `logical opposite` completions.\n",
        "\n",
        "\n",
        "**Affirmative vs. Negated sentences:**\n",
        "- For each \"minimal pair\", we'll want two versions, which begin with an `affirmative` sentence frame (`'A mouse is a'`) or a `negated` sentence frame (`'A mouse is not a'`).\n",
        "- Sentence structures are reused with a word that inverts the logistically opposite pattern.\n",
        "- There are different types completions for each `affirmative` and `negated` sentence frame, which include: `Affirmative-Correct`, `Negated-Anomalous`, `Affirmative-Related`, `Negated-Related`, `Affirmative-Nonrelated`, `Negated-Nonrelated`, `Affirmative-Same`, and `Negated-Same`. `Affirmative-Correct` words are words that correctly categorize the subject. `Negated-Anomalous` words are the same words used in the `Affirmative-Correct`, which should create higher surprisal since the word is not a logical opposite of the subject. `Related` words are words within the same category of the subject, `Nonrelated` words are words that would have no relation to the subject whatsoever.\n",
        "- If the LLM is sensitive to negation, the surprisal relationships between `affirmative` and `negated` sentence frames should reflect the following within the specified minimal pairs:\n",
        "\n",
        "\n",
        "1.   Negated surprisal should be higher than affirmative surprisal: `Affirmative-Correct` and `Negated-Anomalous`, `Affirmative-Same`, and `Negated-Same`\n",
        "2.   Affirmative surprisal should be higher than negated surprisal:\n",
        "`Affirmative-Related` and `Negated-Related`, `Affirmative-Nonrelated` and `Negated-Nonrelated`(although, these two might have similar high surprisal since the subject and unrelated critical word do not commonly appear together)\n",
        "\n",
        "\n",
        "**Possible counfounds or issues:**\n",
        "- Minimal pairs are good for assessing grammatical rules, but they may fall short when assessing meaning during negation since the stimuli will focus on the negation term `not` rather than a general ability to handle negation.\n",
        "- `not` is the only negation term used throughout the stimuli, while it is consistent, it does not necessarily cover all scopes of negation (e.g., \"never\", \"neither\", \"no\", offline methods using \"true\" or \"false\").\n",
        "- Some completions could generate higher surprisal due to being rare rather than being false, which would be a reflection of the training data and not the LLM.\n",
        "- The use of either \"a\" or \"an\" at the end of affirmative and negated sentence frames could introduce surprisal differences that are unrelated to semantic content since GPT-2 might be sensitive to phonological cues.\n",
        "- World Knowledge Violations: even when negation is syntactically correct, the model may assign high probability to a semantically similar word due to frequent associations between the two words.\n",
        "- Human bias: the completions for each conditions are subjective to the researcher, critical word selections may be inconsistent with how they meet their condition.\n"
      ],
      "metadata": {
        "id": "i4mqzAWpkpEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining the stimuli\n",
        "\n",
        "- Each stimulis has a sentence, final completion, and condition label.\n",
        "- The sentences come in *pairs*. This is to make sure we control for everything but the final word.  \n",
        "- The `same word` condition is also used as a control, if the LLM is sensitive to negation, `same word` completions should have similar surprisal to `semantically similar` and `negated-anomalous` completions. This condition will directly measure whether the model suppresses the probability of a semantically affirmed word when it is logically contradicted by negation in the sentence frame. This is important for isolating the use of \"not\" within a sentence frame and determining if differences in surprisal are from an LLM's sensitivity to negation or it's ability to \"pattern match\" and favor repetition."
      ],
      "metadata": {
        "id": "d7VwjK1ym6SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stimuli1 = [\n",
        "    ### sentence, word, condition, item number\n",
        "    ('He is not stupid, he is', ' dumb', 'Semantically Similar'),\n",
        "    ('He is not stupid, he is', ' smart', 'Logical Opposite'),\n",
        "    ('He is not intellegent, he is', ' smart', 'Semantically Similar'),\n",
        "    ('He is not intellegent, he is', ' dumb', 'Logical Opposite'),\n",
        "    ('She is not nice, she is', ' sweet', 'Semantically Similar'),\n",
        "    ('She is not nice, she is', ' mean', 'Logical Opposite'),\n",
        "    ('She is not callous, she is', ' mean', 'Semantically Similar'),\n",
        "    ('She is not callous, he is', ' sweet', 'Logical Opposite'),\n",
        "    ('The weather today is not cloudy, it is', ' foggy', 'Semantically Similar'),\n",
        "    ('The weather today is not cloudy, it is', ' sunny', 'Logical Opposite'),\n",
        "    ('The weather today is not clear, it is', ' sunny', 'Semantically Similar'),\n",
        "    ('The weather today is not clear, it is', ' foggy', 'Logical Opposite'),\n",
        "    ('He is not hardworking, he is', ' determined', 'Semantically Similar'),\n",
        "    ('He is not hardworking, he is', ' lazy', 'Logical Opposite'),\n",
        "    ('He is not idle, he is', ' lazy', 'Semantically Similar'),\n",
        "    ('He is not idle, he is', ' determined', 'Logical Opposite'),\n",
        "    ('She is not shy, she is', ' introverted', 'Semantically Similar'),\n",
        "    ('She is not shy, she is', ' extroverted', 'Logical Opposite'),\n",
        "    ('She is not outgoing, she is', ' extroverted', 'Semantically Similar'),\n",
        "    ('She is not outgoing, she is', ' introverted', 'Logical Opposite'),\n",
        "    ('He is not stupid, he is', ' stupid', 'Same Word'),\n",
        "    ('He is not intellegent, he is', ' intellegent', 'Same Word'),\n",
        "    ('She is not nice, she is', ' nice', 'Same Word'),\n",
        "    ('She is not callous, she is', ' callous', 'Same Word'),\n",
        "    ('The weather today is not cloudy, it is', ' cloudy', 'Same Word'),\n",
        "    ('The weather today is not clear, it is', ' clear', 'Same Word'),\n",
        "    ('He is not hardworking, he is', ' hardworking', 'Same Word'),\n",
        "    ('He is not idle, he is', ' idle', 'Same Word'),\n",
        "    ('She is not shy, she is', ' shy', 'Same Word'),\n",
        "    ('She is not outgoing, she is', ' outgoing', 'Same Word')\n",
        "]"
      ],
      "metadata": {
        "id": "vOaubteDknv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stimuli2 = [\n",
        "    ### sentence, word, condition, item number\n",
        "    ('A mouse is a', ' rodent', 'Affirmative-Correct'),\n",
        "    ('A mouse is not a', ' rodent', 'Negated-Anomalous'),\n",
        "    ('A mouse is a', ' rat', 'Affirmative-Related'),\n",
        "    ('A mouse is not a', ' rat', 'Negated-Related'),\n",
        "    ('A mouse is a', ' cup', 'Affirmative-Nonrelated'),\n",
        "    ('A mouse is not a', ' cup', 'Negated-Nonrelated'),\n",
        "    ('A rose is a', ' flower', 'Affirmative-Correct'),\n",
        "    ('A rose is not a', ' flower', 'Negated-Anomalous'),\n",
        "    ('A rose is a', ' tulip', 'Affirmative-Related'),\n",
        "    ('A rose is not a', ' tulip', 'Negated-Related'),\n",
        "    ('A rose is a', ' spoon', 'Affirmative-Nonrelated'),\n",
        "    ('A rose is not a', ' spoon', 'Negated-Nonrelated'),\n",
        "    ('A dolphin is a', ' mammal', 'Affirmative-Correct'),\n",
        "    ('A dolphin is not a', ' mammal', 'Negated-Anomalous'),\n",
        "    ('A dolphin is a', ' whale', 'Affirmative-Related'),\n",
        "    ('A dolphin is not a', ' whale', 'Negated-Related'),\n",
        "    ('A dolphin is a', ' chair', 'Affirmative-Nonrelated'),\n",
        "    ('A dolphin is not a', ' chair', 'Negated-Nonrelated'),\n",
        "    ('A violin is an', ' instrument', 'Affirmative-Correct'),\n",
        "    ('A violin is not an', ' instrument', 'Negated-Anomalous'),\n",
        "    ('A violin is a', ' viola', 'Affirmative-Related'),\n",
        "    ('A violin is not a', ' viola', 'Negated-Related'),\n",
        "    ('A violin is a', ' human', 'Affirmative-Nonrelated'),\n",
        "    ('A violin is not a', ' human', 'Negated-Nonrelated'),\n",
        "    ('A grape is a', ' fruit', 'Affirmative-Correct'),\n",
        "    ('A grape is not a', ' fruit', 'Negated-Anomalous'),\n",
        "    ('A grape is a', ' blueberry', 'Affirmative-Related'),\n",
        "    ('A grape is not a', ' blueberry', 'Negated-Related'),\n",
        "    ('A grape is a', ' pillow', 'Affirmative-Nonrelated'),\n",
        "    ('A grape is not a', ' pillow', 'Negated-Nonrelated'),\n",
        "    ('A mouse is a', ' mouse', 'Affirmative-Same'),\n",
        "    ('A mouse is not a', ' mouse', 'Negated-Same'),\n",
        "    ('A rose is a', ' rose', 'Affirmative-Same'),\n",
        "    ('A rose is not a', ' rose', 'Negated-Same'),\n",
        "    ('A dolphin is a', ' dolphin', 'Affirmative-Same'),\n",
        "    ('A dolphin is not a', ' dolphin', 'Negated-Same'),\n",
        "    ('A violin is a', ' violin', 'Affirmative-Same'),\n",
        "    ('A violin is not a', ' violin', 'Negated-Same'),\n",
        "    ('A grape is a', ' grape', 'Affirmative-Same'),\n",
        "    ('A grape is not a', ' grape', 'Affirmative-Same'),\n",
        "]"
      ],
      "metadata": {
        "id": "65Ln5YdeqSiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate surprisal\n"
      ],
      "metadata": {
        "id": "1hRohpifnWhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probability Function\n",
        "\n",
        "This function calculates the propabilities of *any* sentence and *any* `candidate`. The inputs include:\n",
        "\n",
        "- A `model` (LLM).\n",
        "- A `tokenizer`.\n",
        "- A text fragment (`seen`).\n",
        "- The thing you want to estimate the probability of (`unseen`).\n",
        "\n",
        "This will also deal with multi-token inputs."
      ],
      "metadata": {
        "id": "E2S471TbiLzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_seq_prob(model, tokenizer, seen, unseen):\n",
        "    \"\"\"Get p(unseen | seen)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : transformers.PreTrainedModel\n",
        "        Model to use for predicting tokens\n",
        "    tokenizer : transformers.PreTrainedTokenizer\n",
        "        Tokenizer for Model\n",
        "    seen : str\n",
        "        Input sequence\n",
        "    unseen: str\n",
        "        The sequence for which to calculate a probability\n",
        "    \"\"\"\n",
        "    # Get ids for tokens\n",
        "    input_ids = tokenizer.encode(seen, return_tensors=\"pt\")\n",
        "    unseen_ids = tokenizer.encode(unseen)\n",
        "\n",
        "    # Loop through unseen tokens & store log probs\n",
        "    log_probs = []\n",
        "    for unseen_id in unseen_ids:\n",
        "\n",
        "        # Run model on input\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids).logits\n",
        "\n",
        "        # Get next token prediction logits\n",
        "        next_token_logits = logits[0, -1]\n",
        "        next_token_probs = torch.softmax(next_token_logits, 0) # Normalize\n",
        "\n",
        "        # Get probability for relevant token in unseen string & store\n",
        "        prob = next_token_probs[unseen_id]\n",
        "        log_probs.append(torch.log(prob))\n",
        "\n",
        "        # Add input tokens incrementally to input\n",
        "        input_ids = torch.cat((input_ids, torch.tensor([[unseen_id]])), 1)\n",
        "\n",
        "    # Add log probs together to get total log probability of sequence\n",
        "    total_log_prob = sum(log_probs)\n",
        "    # Exponentiate to return to probabilities\n",
        "    total_prob = torch.exp(total_log_prob)\n",
        "    return total_prob.item()"
      ],
      "metadata": {
        "id": "tRrEuUBoiLKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Surprisal Function\n",
        "\n",
        "> The **surprisal** of a token is its negative log probability. Higher \"surprisal\" corresponds to lower probability, i.e., the model is more \"surprised\" by a given word."
      ],
      "metadata": {
        "id": "up-gpItGjtvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def surprisal(p):\n",
        "  return -np.log2(p)"
      ],
      "metadata": {
        "id": "WiMHQ3bsjg_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obtain surprisal values"
      ],
      "metadata": {
        "id": "WNJrd4CVkH8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create DataFrame for Stimuli 1\n",
        "df_stimuli1 = pd.DataFrame(stimuli1, columns = ['Sentence', 'Word', 'Condition'])\n",
        "df_stimuli1.head(3)\n",
        "\n",
        "### Obtain Surprisal using the next_seq_prob() function\n",
        "results1 = []\n",
        "for index, row in tqdm(df_stimuli1.iterrows(), total=df_stimuli1.shape[0]):\n",
        "\n",
        "  prob = next_seq_prob(gpt2, tokenizer, row['Sentence'], row['Word'])\n",
        "\n",
        "  results1.append({\n",
        "      'Word': row['Word'],\n",
        "      'Sentence': row['Sentence'],\n",
        "      'Condition': row['Condition'],\n",
        "      'Probability': prob,\n",
        "      'Surprisal': surprisal(prob)\n",
        "  })"
      ],
      "metadata": {
        "id": "EUUp8Gzyi7v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create DataFrame for Stimuli 2\n",
        "df_stimuli2 = pd.DataFrame(stimuli2, columns = ['Sentence', 'Word', 'Condition'])\n",
        "\n",
        "### Obtain Surprisal using the next_seq_prob() function\n",
        "results2 = []\n",
        "for index, row in tqdm(df_stimuli2.iterrows(), total=df_stimuli2.shape[0]):\n",
        "\n",
        "  prob = next_seq_prob(gpt2, tokenizer, row['Sentence'], row['Word'])\n",
        "\n",
        "  results2.append({\n",
        "      'Word': row['Word'],\n",
        "      'Sentence': row['Sentence'],\n",
        "      'Condition': row['Condition'],\n",
        "      'Probability': prob,\n",
        "      'Surprisal': surprisal(prob)\n",
        "  })"
      ],
      "metadata": {
        "id": "qMV6ky-mzKEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizations"
      ],
      "metadata": {
        "id": "xTQYY1pU_cWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Create Results DataFrame\n",
        "df_results1 = pd.DataFrame(results1)\n",
        "print(df_results1.head(2))\n",
        "print(df_results1.tail(2))"
      ],
      "metadata": {
        "id": "-CMvtg_xAXqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Create Results DataFrame\n",
        "df_results2 = pd.DataFrame(results2)\n",
        "print(df_results2.head(6))\n",
        "print(df_results2[30:32])"
      ],
      "metadata": {
        "id": "z4IDvs_iAfX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Semantically similar words vs. Logical opposites barplot\n",
        "sns.barplot(data = df_results1, x = \"Condition\", y = \"Surprisal\").set(title=\"Semantically similar words vs. Logical opposites\")"
      ],
      "metadata": {
        "id": "a6P1Lyc1_qpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Affirmative vs. Negated sentences barplot\n",
        "sns.barplot(data = df_results2, x = \"Condition\", y = \"Surprisal\").set(title=\"Affirmative vs. Negated sentences\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "N4CW4wXd_YO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "\n",
        "The surprisal for `semantically similar` completions was slightly larger than `logical opposite` completions. We predicted that this would occur if GPT-2 was sensitive to negation, but the `same word` completions had the smallest surprisal out of all the conditions, differing from their predicted surprisal (a larger surprisal than `logical opposite` completions).\n",
        "\n",
        "This was also true for the `affirmative` vs. `negated` sentence frames, where `same` word completions had the smallest surprisal out of all the conditions. For all of the `affirmative` vs. `negated` sentence frame conditions, the words in the `affirmative` conditions had higher surprisal than the words in the `negated` conditions, despite different semantic relations to the target word. The `non-related` words had the largest surprisal values which was not surprising, but it is hard to tell whether the high surprisal is a result of negation sensitivity or since they were the most likely words to not be associated with the subject in the sentence frame in the training data, or in real life. The `affirmative-correct` and `negated-anomalous` completions produced the second smallest surprisals out of the minimal pairs addressing the sentence frames, which meets expectations as producing a low surprisal for correctly phrased negation is the \"expected\" behavior for an LLM that is sensitive to negation."
      ],
      "metadata": {
        "id": "gYdLgdFXxzQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "The results indicate that GPT-2 exhibits some limited sensitivity to semantic relationships under negation; however, it does not consistently demonstrate sensitivity to the logical meaning of negation itself. Specifically, surprisal was slightly higher for semantically similar completions compared to logical opposite completions, aligning with our prediction if the model was sensitive to negation. However, contrary to predictions, the surprisal for completions with the same word as the subject is lower than both of these conditions. This suggests that the differences may be due to token familiarity and frequency rather than logical reasoning for negation. This can also be inferred from the results of using affirmative and negated sentence frames, where broader yet semantically related words produced higher surprisal within affirmative conditions rather than negated conditions. Overall, these findings suggest that GPT-2 does not seem to be fully sensitive to negation in the way that humans are.\n",
        "\n",
        "\n",
        "These results reflect the capabilities of large language models to capture different dimensions of meaning with distributional semantics, specifically their ability to understand logic-based semantics. LLMs such as GPT-2 may struggle with generalization when using logic-based rules; they tend to ignore how terms with logical rules can alter sentence meaning and focus more on predicting words that are commonly found within the same context. This suggests that even pretrained LLMs may lack a systemic understanding of logic-based rules, instead relying on surface-level statistical associations learned during training.\n",
        "\n",
        "The mismatch between GPT-2's performance on negation tasks and human expectations calls into question what information is necessary for logic-based semantic understanding. Although negation is a frequent construction in language, it is hard to capture the information needed to structure the logic of the term when an understanding of what a subject \"is not\" requires a vast understanding of what an object \"is\". Humans perform similarly to LLMs when asked to \"not\" think of something and tend to think about the subject they were asked not to think about. This doesn't imply that humans do not understand negation, but rather that thinking about a logical opposite in an open-ended manner is a hard task to accomplish due to the infinite amount of words that could qualify as logical opposites. Despite this, humans are more capable of reflective responses to negation and being able to reason through their answers. This may be reflected in more modern LLMs, where negation sensitivity may be higher due to their training shifting away from shallow processing and more towards chain-of-thought reasoning."
      ],
      "metadata": {
        "id": "L-tYcG-wyv9Z"
      }
    }
  ]
}